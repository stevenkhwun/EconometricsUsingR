---
title: "Simple Regression Analysis"
author: "SW"
date: "2022/7/10"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Key terms

* Dependent variable
* Explanatory variable, independent variable, regressor
* Disturbance term
* Residual
* Residual sum of squares, _RSS_
* Ordinary least squares, _OLS_
* Normal equations
* Demeaning


## 1. The simple linear model

**The model**:

$$Y_{i}=\beta_{1}+\beta_{2}X_{i}+u_{i}$$
where $Y$ is usually described as the **dependent variable**, and $X$ as the **explanatory variable** or **independent variable** or the **regressor**. In statistical analysis, one generally acknowledges the fact that the relationship is not exact by explicitly including in it a random factor known as the **disturbance term**, $u_i$.

**The fitted model**:

$$\hat{Y_{i}}=\hat{\beta_{1}}+\hat{\beta_{2}}X_{i}$$

The difference between the actual value of $Y_{i}$ and the fitted value $\hat{Y_{i}}$, in observation $i$ is known as the **residual** in the observation $i$. It will be denoted \hat{\mu_{i}}:

$$\hat{u}_{i}=Y_{i}-\hat{Y_{i}}$$


## 2. Derivation of the regression coefficients

We use the **least squares** criterion to choose $\hat{\beta_{1}}$ amd $\hat{\beta_{2}}$ so as to minimize _**RSS**_, the **residual sum of squares** (sum of the squares of the residuals).

$$RSS=\sum_{i=1}^{n}{\hat{u}_{i}}^2$$
This is usually referred to as **ordinary least squares** and abbreviated _**OLS**_.  
<br />
The square of the residual in observation $i$ in terms of $\hat{\beta_{1}}$ and $\hat{\beta_{2}}$, and the data on $X$ and $Y$:


\begin{eqnarray}
{\hat{u}_{i}}^2 &=&(Y_i-\hat{Y}_{i})^2 =(Y_i-\hat{\beta}_{1}-\hat{\beta}_{2}X_i)^2 \\

&=& Y_i^2+\hat{\beta}_{1}^2+\hat{\beta}_{2}^2X_i^2-2\hat{\beta}_{1}Y_i-2\hat{\beta}_{2}X_iY_i+2\hat{\beta}_{1}\hat{\beta}_{2}X_i \\
\end{eqnarray}


<br />


Summing over all the $n$ observations, we can write _RSS_ as

$$RSS=\sum_{i=1}^{n}Y_i^2+n\hat{\beta}_{1}^2+\hat{\beta}_{2}^2\sum_{i=1}^{n}X_i^2-2\hat{\beta}_{1}\sum_{i=1}^{n}Y_i-2\hat{\beta}_{2}\sum_{i=1}^{n}X_iY_i+2\hat{\beta}_{1}\hat{\beta}_{2}\sum_{i=1}^{n}X_i$$

We find the particular values of $\hat{\beta}_{1}$ and $\hat{\beta}_{2}$ by minimize _RSS_. The partial differentials of _RSS_ with respect to $\hat{\beta}_{1}$ and $\hat{\beta}_{2}$ are:

$$\frac{\partial{RSS}}{\partial{\hat{\beta}_{1}}}=2n\hat{\beta}_{1}-2\sum_{i=1}^{n}Y_i+2\hat{\beta}_{2}\sum_{i=1}^{n}X_i$$

$$\frac{\partial{RSS}}{\partial{\hat{\beta}_{2}}}=2\hat{\beta}_{2}\sum_{i=1}^{n}X_i^2-2\sum_{i=1}^{n}X_iY_i+2\hat{\beta}_{1}\sum_{i=1}^{n}X_i$$

The values of $\hat{\beta}_{1}$ and $\hat{\beta}_{2}$ that minimize _RSS_ must satisfy the first-order conditions

\begin{equation}
\frac{\partial{RSS}}{\partial{\hat{\beta}_{1}}}=0
\quad\mathrm{and}\quad
\frac{\partial{RSS}}{\partial{\hat{\beta}_{2}}}=0
\end{equation}



Hence
$$2n\hat{\beta}_{1}-2\sum_{i=1}^{n}Y_i+2\hat{\beta}_{2}\sum_{i=1}^{n}X_i=0$$
$$2\hat{\beta}_{2}\sum_{i=1}^{n}X_i^2-2\sum_{i=1}^{n}X_iY_i+2\hat{\beta}_{1}\sum_{i=1}^{n}X_i=0$$
These equations are known as the **normal equations** for the regression coefficients.  
<br />
Solving the normal equations and noting that

\begin{equation}
\overline{X}=\frac{1}{n}\sum_{i=1}^{n}X_i
\quad\mathrm{and}\quad
\overline{Y}=\frac{1}{n}\sum_{i=1}^{n}Y_i
\end{equation}

and hence

$$\hat{\beta}_{1}=\overline{Y}-\hat{\beta}_{2}\overline{X}$$
$$\hat{\beta}_{2}=\frac{\sum_{i=1}^{n}X_iY_i-n\overline{X}\overline{Y}}{\sum_{i=1}^{n}X_i^2-n\overline{X}^2}=\frac{\sum_{i=1}^{n}(X_i-\overline{X})(Y_i-\overline{Y})}{\sum_{i=1}^{n}(X_i-\overline{X})^2}$$






## 3. An example

The following example is adopted from an example in p. 98 of the text book. Hourly earnings in 2011, `EARNINGS`, measured in dollars, are regressed on schooling, `S`, measured as highest grade completed, for 500 respondents from the US National Longitudinal Survey of Youth 1997-.
```{r}
# EARNINGS regress on schooling
eawe21 <- read.csv('./Data/EAWE21.csv', header=T)   # Import data
lin.model <- lm(EARNINGS ~ S, data = eawe21)        # Run the model
summary(lin.model)                                  # The result
```

The regression output gives the estimates of the intercept and the coefficient of `S`, and thus the following fitted equation:

$$\hat{EARNINGS}=0.76+1.27S$$
Interpreting the equation literally, the slope coefficient indicates that, as `S` increases by one unit, `EARNINGS` increases by 1.27 units. This implies that hourly earnings increase by $1.27 for every extra year of schooling.

#### Demeaning
Often the intercept in a regression equation has no sensible interpretation because $X=0$ is distant from the data range. The wage equation above is an example, with the intercept being implausibly low.  
<br />
Sometimes it is useful to deal with the problem by defining $X^*$ as the deviation of $X$ about its sample mean

$$X_{i}^{*}=X_i-\overline{X}$$
and regressing $Y$ on $X^*$. The slope coefficient will not be affected, but the intercept will now be the fitted value of $Y$ at the sample mean of $X$.  
<br />
The following R code gives the descriptive statistics of `S`. This gives the mean years of schooling was 14.87.
```{r}
# Descriptive statistics for S
summary(eawe21$S)
```
We then regress `EARNINGS` on the demeaned variable `SDEV`, where
$$SDEV_i=S_i-\overline{S}$$
```{r, message=FALSE, warning=FALSE}
# Create the demeaned variable SDEV
library(tidyverse)                  # Load the required package

eawe21.dm <- eawe21 %>%
  mutate(SDEV = S - mean(S)) %>%    # Create the demeaned variable
  select(EARNINGS, S, SDEV)         # Select the required columns

# EARNINGS regress on the demeaned variable SDEV
# Using pipe (%>%) operator to simplify the code
lm(EARNINGS ~ SDEV, data = eawe21.dm) %>%        # Run the model
  summary                                        # Print the result      
```
From the regression results, we can see that the coefficient for `SDEV` is 1.27, which is the same as the coefficient of `S` in the original wage equation. However, this regression further predicts that the hourly earnings is $19.58 for the mean years of schooling of 14.87.


## 4. Two important results relating to OLS regressions

These results are valid only if the model includes an intercept.

1.  The mean value of the residuals is zero
$$\overline{\hat{u}}=0$$
As a corollary, the mean of the fitted values of $Y$ is equal to the mean of the actual values of $Y$.
$$\overline{\hat{Y}}=\overline{Y}$$
2.  The sample correlation between the observations on $X$ and the residuals is zero  
Using the normal equation, we can show that $\sum{}{}X_i\hat{u}_i=0$. Together with the fact that $\overline{\hat{u}}=0$, the numerator of the sample correlation coefficient for $X$ and $\hat{u}$ is $0$, that is:
$$\frac{1}{n}\sum_{i=1}^{n}(X_i-\overline{X})(\hat{u}_i-\overline{\hat{u}})=0$$
Hence the correlation coefficient is zero, provided that the denominator is nonzero, which means that the sample variances of $X$ and $\hat{u}$ are both nonzero.  
As a corollary, the fitted values of the dependent variable are uncorrelated with the residuals in a simple regression model.

```{r}
# EARNINGS regress on schooling
eawe21 <- read.csv('./Data/EAWE21.csv', header=T)   # Import data
linear_model <- lm(EARNINGS ~ S, data = eawe21)     # Run the model
summary(linear_model)
anova(linear_model)
```

```{r}
# EARNINGS regress on schooling
eawe21 <- read.csv('./Data/EAWE21.csv', header=T)   # Import data
lm(EARNINGS ~ S, data = eawe21) %>%
  summary.lm()
```


