---
title: "Simple Regression Analysis"
author: "AW"
date: "2022/7/10"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Key terms

* Dependent vaiable
* Expanatory variable, independent variable, regressor
* Disturbance term
* Residual
* Residual sum of squares, _RSS_
* Ordinary least squuares, _OLS_
* Normal equations


## 1. The simple linear model

**The model**:

$$Y_{i}=\beta_{1}+\beta_{2}X_{i}+u_{i}$$
where $Y$ is usually described as the **dependent variable**, and $X$ as the **explanatory variable** or **independent variable** or the **regressor**. In statistical analysis, one generally acknowledgesthe fact that the relationship is not exact by explicitly including in it a random factor known as the **disturbance term**, $\mu_{i}$.

**The fitted model**:

$$\hat{Y_{i}}=\hat{\beta_{1}}+\hat{\beta_{2}}X_{i}$$

The difference between the actual value of $Y_{i}$ and the fitted value $\hat{Y_{i}}, in observation $i$ is known as the **residual** in the observation $i$. It will be denoted \hat{\mu_{i}}:

$$\hat{u}_{i}=Y_{i}-\hat{Y_{i}}$$


## 2. Derivation of the regression coefficients

We use the **least squares** criterion to choose $\hat{\beta_{1}}$ amd $\hat{\beta_{2}}$ so as to minimize _**RSS**_, the **residual sum of squares** (sum of the squares of the residuals).

$$RSS=\sum_{i=1}^{n}{\hat{u}_{i}}^2$$
This is usually referred to as **ordinary least squares** and abbreviated _**OLS**_.  
<br />
The square of the residual in observaiont $i$ interms of $\hat{\beta_{1}}$ and \hat{\beta_{2}}$, and the data on $X$ and $Y$:


\begin{eqnarray}
{\hat{u}_{i}}^2 &=&(Y_i-\hat{Y}_{i})^2 =(Y_i-\hat{\beta}_{1}-\hat{\beta}_{2}X_i)^2 \\

&=& Y_i^2+\hat{\beta}_{1}^2+\hat{\beta}_{2}^2X_i^2-2\hat{\beta}_{1}Y_i-2\hat{\beta}_{2}X_iY_i+2\hat{\beta}_{1}\hat{\beta}_{2}X_i \\
\end{eqnarray}

(For formatting of the above equation array, see the website [Using R Markdown for Class Reports][Eqnarray] by Cosma Shalizi for more information.)  
<br />


Summing over all the $n$ observations, we can write _RSS_ as

$$RSS=\sum_{i=1}^{n}Y_i^2+n\hat{\beta}_{1}^2+\hat{\beta}_{2}^2\sum_{i=1}^{n}X_i^2-2\hat{\beta}_{1}\sum_{i=1}^{n}Y_i-2\hat{\beta}_{2}\sum_{i=1}^{n}X_iY_i+2\hat{\beta}_{1}\hat{\beta}_{2}\sum_{i=1}^{n}X_i$$

We find the particular values of $\hat{\beta}_{1}$ and $\hat{\beta}_{2}$ by minimize _RSS_. The partial differentials of _RSS_ with respect to $\hat{\beta}_{1}$ and $\hat{\beta}_{2}$ are:

$$\frac{\partial{RSS}}{\partial{\hat{\beta}_{1}}}=2n\hat{\beta}_{1}-2\sum_{i=1}^{n}Y_i+2\hat{\beta}_{2}\sum_{i=1}^{n}X_i$$

$$\frac{\partial{RSS}}{\partial{\hat{\beta}_{2}}}=2\hat{\beta}_{2}\sum_{i=1}^{n}X_i^2-2\sum_{i=1}^{n}X_iY_i+2\hat{\beta}_{1}\sum_{i=1}^{n}X_i$$

The values of $\hat{\beta}_{1}$ and $\hat{\beta}_{2}$ that minimize _RSS_ must satisfy the first-order conditions

\begin{equation}
\frac{\partial{RSS}}{\partial{\hat{\beta}_{1}}}=0
\quad\mathrm{and}\quad
\frac{\partial{RSS}}{\partial{\hat{\beta}_{2}}}=0
\end{equation}

(Please see the website [https://tex.stackexchange.com/questions/288222/two-equations-in-one-line][2equations] for more information on how to format two equations on one line.)

Hence
$$2n\hat{\beta}_{1}-2\sum_{i=1}^{n}Y_i+2\hat{\beta}_{2}\sum_{i=1}^{n}X_i=0$$
$$2\hat{\beta}_{2}\sum_{i=1}^{n}X_i^2-2\sum_{i=1}^{n}X_iY_i+2\hat{\beta}_{1}\sum_{i=1}^{n}X_i=0$$
These equations are known as the **normal equations** for the regression coefficients.  
<br />
Solving the normal equations and noting that

\begin{equation}
\bar{X}=\frac{1}{n}\sum_{i=1}^{n}X_i
\quad\mathrm{and}\quad
\bar{Y}=\frac{1}{n}\sum_{i=1}^{n}Y_i
\end{equation}

and hence

$$\hat{\beta}_{1}=\bar{Y}-\hat{\beta}_{2}\bar{X}$$






## 3. An example
```{r}
# Example in page 98 of the text
eawe21 <- read.csv('./Data/EAWE21.csv', header=T)   # Import data
linear_model <- lm(EARNINGS ~ S, data = eawe21)     # Run the model
summary(linear_model)                               # The result
```

```{r}
summary(eawe21$S)
```

```{r, message=FALSE, warning=FALSE}
# Create the demeaned variable SDEV
library(tidyverse)                  # Load the required package

eawe21_dm <- eawe21 %>%
  mutate(SDEV = S - mean(S)) %>%    # Create the demeaned variable
  select(EARNINGS, S, SDEV)         # Select the required columns

# EARNINGS regress on the demeaned variable SDEV
linear_model_dm <- lm(EARNINGS ~ SDEV, data = eawe21_dm)     # Run the model
summary(linear_model_dm)                                     # The result      
```


[Eqnarray]: https://www.stat.cmu.edu/~cshalizi/rmarkdown/
[2equations]: https://tex.stackexchange.com/questions/288222/two-equations-in-one-line
