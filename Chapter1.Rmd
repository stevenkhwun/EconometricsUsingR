---
title: "Simple regression model"
output: bookdown::html_document2
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

**Key terms**

* Dependent variable
* Explanatory variable, independent variable, regressor
* Error term, disturbance
* Residual
* Residual sum of squares, _RSS_
* Ordinary least squares, _OLS_
* Normal equations
* Demeaning
* Goodness of fit
* Total sum of squares, _TSS_
* Explained sum of squares, _ESS_
* Coefficient of determination: $R^2$


# The simple regression model

## The model

Equation \@ref(eq:slm), which is assumed to hold in the population of interest, defines the **simple linear regression model**.

\begin{equation}
y=\beta_{0}+\beta_{1}x+u
(\#eq:slm)
\end{equation}


where $y$ is usually described as the **dependent variable**, and $x$ as the **explanatory variable** or **independent variable** or the **regressor**. In statistical analysis, one generally acknowledges the fact that the relationship is not exact by explicitly including in it a random factor known as the **error term** or **disturbance**, $u$.\
\
The simple regression model implies that $x$ has a linear effect on $y$:
$${\Delta}y = \beta_{1}{\Delta}x$$
if ${\Delta}u=0$.\
\
In other words, the marginal effect of $x$ on $y$ is constant and equal to $\beta_1$.


## The fitted model

After the parameters, $\beta_0$ and $\beta_1$, are estimated, the fitted model can be represented as follows:
\begin{equation}
\hat{y_{i}}=\hat{\beta_{0}}+\hat{\beta_{1}}x_{i}
(\#eq:slm-fitted)
\end{equation}

The difference between the actual value of $y_{i}$ and the fitted value $\hat{y_{i}}$, in observation $i$ is known as the **residual** in the observation $i$, $\hat{u}_{i}$. It can be expressed as:

\begin{equation}
\hat{u}_{i}=y_{i}-\hat{y_{i}}
(\#eq:residual)
\end{equation}

## Assumptions of the error term, $u$
There are two assumptions on the *error term* of the model:

1.  As long as the intercept $\beta_{0}$ is included in the equation, we assume
\begin{equation}
E(u)=0
(#eq:u-A1)
\end{equation}
This is an innocuous normalisation and can be imposed without a loss of generality. However, it says nothing about the relationship between $u$ and $x$.

2.  The second key assumption is on the expected vaue of $u$ given $x$ or, in other words, on the mean of the error term for each slice of the population determined by the values of $x$, that is:
\begin{equation}
E(u|x)=E(u) ~~ for ~~ all ~~ values ~~ of ~~ x
(#eq:u-A2)
\end{equation}
This implies that $u$ is **mean independent** of $x$, or $u$ is uncorrelated with $x$.\
\
Combining with the first assumption, we have
\begin{equation}
E(u|x)=E(u)=0 ~~ for ~~ all ~~ values ~~ of ~~ x
(#eq:u-A2-1)
\end{equation}
This is called the **zero conditional mean assumption**. It says that the expected value of the error term, $u$, is zero, regardless of what the value of the explanatory variable, $x$, is.


By the assumptions of the *error terms*, especially the property of the conditional expectation, $E(u|x)=0$, implies:

\begin{eqnarray}
E(y|x) &=& E(\beta_0+{\beta_1}x+u|x) \\
&=& \beta_0+{\beta_1}x+E(u|x) \\
&=& \beta_0+{\beta_1}x \\
(#eq:prf)
\end{eqnarray}

This shows that the **population regression function**, $E(y|x)$, has the following properties:

*  it is a linear function of x
*  an one-unit change in $x$ changes the expected value of $y$ by the amount of $\beta_1$
*  the distribution of $y$ is centered about $E(y|x)$
*  it gives us a relationship between the average level of $y$ at different levels of $x$


# Derivation of the regression coefficients

## Least squares approach

We use the **least squares** criterion to choose $\hat{\beta_{1}}$ amd $\hat{\beta_{2}}$ so as to minimize _**RSS**_, the **residual sum of squares** (sum of the squares of the residuals).

$$RSS=\sum_{i=1}^{n}{\hat{u}_{i}}^2$$
This is usually referred to as **ordinary least squares** and abbreviated _**OLS**_.  
<br />
The square of the residual in observation $i$ in terms of $\hat{\beta_{1}}$ and $\hat{\beta_{2}}$, and the data on $X$ and $Y$:


\begin{eqnarray}
{\hat{u}_{i}}^2 &=&(Y_i-\hat{Y}_{i})^2 =(Y_i-\hat{\beta}_{1}-\hat{\beta}_{2}X_i)^2 \\

&=& Y_i^2+\hat{\beta}_{1}^2+\hat{\beta}_{2}^2X_i^2-2\hat{\beta}_{1}Y_i-2\hat{\beta}_{2}X_iY_i+2\hat{\beta}_{1}\hat{\beta}_{2}X_i \\
\end{eqnarray}


<br />


Summing over all the $n$ observations, we can write _RSS_ as

$$RSS=\sum_{i=1}^{n}Y_i^2+n\hat{\beta}_{1}^2+\hat{\beta}_{2}^2\sum_{i=1}^{n}X_i^2-2\hat{\beta}_{1}\sum_{i=1}^{n}Y_i-2\hat{\beta}_{2}\sum_{i=1}^{n}X_iY_i+2\hat{\beta}_{1}\hat{\beta}_{2}\sum_{i=1}^{n}X_i$$

We find the particular values of $\hat{\beta}_{1}$ and $\hat{\beta}_{2}$ by minimize _RSS_. The partial differentials of _RSS_ with respect to $\hat{\beta}_{1}$ and $\hat{\beta}_{2}$ are:

$$\frac{\partial{RSS}}{\partial{\hat{\beta}_{1}}}=2n\hat{\beta}_{1}-2\sum_{i=1}^{n}Y_i+2\hat{\beta}_{2}\sum_{i=1}^{n}X_i$$

$$\frac{\partial{RSS}}{\partial{\hat{\beta}_{2}}}=2\hat{\beta}_{2}\sum_{i=1}^{n}X_i^2-2\sum_{i=1}^{n}X_iY_i+2\hat{\beta}_{1}\sum_{i=1}^{n}X_i$$

The values of $\hat{\beta}_{1}$ and $\hat{\beta}_{2}$ that minimize _RSS_ must satisfy the first-order conditions

\begin{equation}
\frac{\partial{RSS}}{\partial{\hat{\beta}_{1}}}=0
\quad\mathrm{and}\quad
\frac{\partial{RSS}}{\partial{\hat{\beta}_{2}}}=0
\end{equation}



Hence
$$2n\hat{\beta}_{1}-2\sum_{i=1}^{n}Y_i+2\hat{\beta}_{2}\sum_{i=1}^{n}X_i=0$$
$$2\hat{\beta}_{2}\sum_{i=1}^{n}X_i^2-2\sum_{i=1}^{n}X_iY_i+2\hat{\beta}_{1}\sum_{i=1}^{n}X_i=0$$
These equations are known as the **normal equations** for the regression coefficients.  
<br />
Solving the normal equations and noting that

\begin{equation}
\overline{X}=\frac{1}{n}\sum_{i=1}^{n}X_i
\quad\mathrm{and}\quad
\overline{Y}=\frac{1}{n}\sum_{i=1}^{n}Y_i
\end{equation}

and hence

\begin{equation}
\hat{\beta}_{0}=\overline{Y}-\hat{\beta}_{1}\overline{X}
(\#eq:intercept)
\end{equation}

\begin{equation}
\hat{\beta}_{1}=\frac{\sum_{i=1}^{n}X_iY_i-n\overline{X}\overline{Y}}{\sum_{i=1}^{n}X_i^2-n\overline{X}^2}=\frac{\sum_{i=1}^{n}(X_i-\overline{X})(Y_i-\overline{Y})}{\sum_{i=1}^{n}(X_i-\overline{X})^2}
(\#eq:slope)
\end{equation}

## Method of moments approach





# An example

The following example is adapted from an example in p. 98 of [@Dougherty]. The variable `EARNING`, hourly earnings in 2011 measured in dollars, is regressed on schooling, `S`, measured as highest grade completed, for 500 respondents from the US National Longitudinal Survey of Youth 1997-.
```{r}
# EARNINGS regress on S
eawe21 <- read.csv('Data/EAWE21.csv', header=T)     # Import data
lin.model <- lm(EARNINGS ~ S, data = eawe21)        # Run the model
summary(lin.model)                                  # The result
```

The regression output gives the estimates of the intercept and the coefficient of `S`, and thus the following fitted equation:

$$\hat{EARNINGS}=0.76+1.27S$$
Interpreting the equation literally, the slope coefficient indicates that, as `S` increases by one unit, `EARNINGS` increases by 1.27 units. This implies that hourly earnings increase by $1.27 for every extra year of schooling.  
<br />
Figure \@ref(fig:simple-wage), which is produced by the following `R` codes, is  a plot of the data together with the regression line. It is a replicate of Figure 1.7 on p. 99 of [@Dougherty].

```{r simple-wage, fig.cap = "A simple wage equation"}
# EARNINGS regress on S
# Plot the data
plot(EARNINGS ~ S, 
     data = eawe21,
     main = "A simple wage equation", 
     xlab = "Years of schooling (highest grade completed)",
     ylab = "Hourly earnings ($)",
     xlim = c(0, 20),
     ylim = c(0, 120))

abline(lin.model)              # Add the regression line

```


## Demeaning
Often the intercept in a regression equation has no sensible interpretation because $X=0$ is distant from the data range. The wage equation above is an example, with the intercept being implausibly low.  
<br />
Sometimes it is useful to deal with the problem by defining $X^*$ as the deviation of $X$ about its sample mean

$$X_{i}^{*}=X_i-\overline{X}$$
and regressing $Y$ on $X^*$. The slope coefficient will not be affected, but the intercept will now be the fitted value of $Y$ at the sample mean of $X$.  
<br />
The following R code gives the descriptive statistics of `S`. This gives the mean years of schooling was 14.87.
```{r}
# Descriptive statistics for S
summary(eawe21$S)
```
We then regress `EARNINGS` on the demeaned variable `SDEV`, where
$$SDEV_i=S_i-\overline{S}$$
```{r, message=FALSE, warning=FALSE}
# Create the demeaned variable SDEV
library(tidyverse)                  # Load the required package

eawe21.dm <- eawe21 %>%
  mutate(SDEV = S - mean(S)) %>%    # Create the demeaned variable
  select(EARNINGS, S, SDEV)         # Select the required columns

# EARNINGS regress on the demeaned variable SDEV
# Using pipe (%>%) operator to simplify the code
lm(EARNINGS ~ SDEV, data = eawe21.dm) %>%        # Run the model
  summary                                        # Print the result      
```
From the regression results, we can see that the coefficient for `SDEV` is 1.27, which is the same as the coefficient of `S` in the original wage equation. However, this regression further predicts that the hourly earnings is $19.58 for the mean years of schooling of 14.87.


# Some important results relating to OLS regressions

These results are valid only if the model includes an intercept. These results hold automatically, irrespective of whether the model is well or poorly specified.

1.  The mean value of the residuals is zero
$$\overline{\hat{u}}=0$$
As a corollary, the mean of the fitted values of $Y$ is equal to the mean of the actual values of $Y$.
$$\overline{\hat{Y}}=\overline{Y}$$
2.  The sample covariance^[By definition, $Cov(X,Y)=E[(X-E[X])(Y-E[Y])]$. Hence, $Cov(X,\hat{u})=E[(X-E[X])(\hat{u}-E[\hat{u}])]=\frac{1}{n}\sum_{i=1}^{n}[(X_i-E[X])(\hat{u}_i-E(\hat{u})]$.] between the observations on $X$ and the residuals is zero. 
Using the normal equation, we can show that $\sum{}{}X_i\hat{u}_i=0$. Together with the fact that $\overline{\hat{u}}=0$, we can show that the sample covariance between $X$ and $\hat{u}$ is $0$, that is:
$$\frac{1}{n}\sum_{i=1}^{n}(X_i-\overline{X})(\hat{u}_i-\overline{\hat{u}})=0$$
Hence the correlation coefficient^[The correlation of two random variables $X$ and $Y$, denoted by $\rho(X,Y)$, is defined as $\rho(X,Y)=\frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}$] is also zero, provided that the sample variances of $X$ and $\hat{u}$ are both nonzero.  
As a corollary, the fitted values of the dependent variable are uncorrelated with the residuals in a simple regression model.

3.  The fitted line must pass through the point {$\overline{X}$,$\overline{Y}$} representing the mean of the observations in the sample.


# Goodness of fit

## Coefficient of determination $R^2$
The variations in $Y$ in any sample can be summarized by
$$TSS=\sum(Y_i-\overline{Y})^2$$
where **_TSS_** is the **total sum of squares**, the sum of the squared deviations about its sample mean.  
<br />
After fitting a regression:
$$Y_i=\hat{Y}_i+\hat{u}_i$$
We can use this to decompose $\sum(Y_i-\overline{Y})^2$:
$$\sum_{i=1}^{n}(Y_i-\overline{Y})^2=\sum_{i=1}^{n}([\hat{Y}_i+\hat{u}_i]-[\overline{\hat{Y}}+\overline{\hat{u}}])^2$$
After some algebraic manipulation, we have:
$$\sum_{i=1}^{n}(Y_i-\overline{Y})^2=\sum_{i=1}^{n}(\hat{Y}_i-\overline{Y})^2+\sum_{i=1}^{n}\hat{u}_i^2$$
Thus we have the decomposition
$$TSS=ESS+RSS$$
where **_ESS_**, the **explained sum of squares**, and **_RSS_**, the **residual sum of squares**, are the two terms on the right side.  
<br />


The proportion of the total sum of squares explained by the regression line is known as the **coefficient of determination** or, more usually, $R^2$:

$$R^2=\frac{ESS}{TSS}=\frac{\sum_{i=1}^{n}(\hat{Y}_i-\overline{Y})^2}{\sum_{i=1}^{n}(Y_i-\overline{Y})^2}$$
The value of _ESS_ and _RSS_ are provided by the command `anova()` in `R`. 
```{r}
# EARNINGS regress on S
# Analysis of variance table
anova(lin.model)
```
_ESS_, which is described as the independent variable `S` sum of squares, is $6,014$. _TSS_ is the sum of _ESS_ and the _RSS_, which is described as `Residual` sum of squares, is 64,315. Hence, $TSS=6,014+64,315=70,329$. 
Dividing _ESS_ by _TSS_, we have $R^2=0.08551$, which is given by the command `summary()` in `R`.
```{r}
# EARNINGS regress on S
# Summary of regression results, including the coefficient of determination
summary(lin.model)
```
Other things being equal, one would like $R^2$ to be as high as possible. In particular, we would like the coefficients $\hat{\beta}_1$ and $\hat{\beta}_2$ should be chosen in such a way as to maximize $R^2$. On the other hand, the coefficients $\hat{\beta}_1$ and $\hat{\beta}_2$ should be chosen to minimize the sum of the squares of the residuals. In fact, the two criteria are equivalent since we can rewrite $R^2$ as:
$$R^2=1-\frac{RSS}{TSS}=1-\frac{\sum_{i=1}^{n}\hat{u}_i^2}{\sum_{i=1}^{n}(Y_i-\overline{Y})^2}$$

Since $\hat{\beta}_1$ and $\hat{\beta}_2$ minimize the residual sum of squares, they automatically maximize $R^2$.  
<br />

## Alternative interpretation of $R^2$
It should be intuitively obvious that the better is the fit achieved by the regression equation, the higher should be the correlation coefficient for the actual and predicted values of $Y$, which the correlation coefficient is denoted as $r_{Y,\hat{Y}}$:

$$r_{Y,\hat{Y}}=\frac{\sum_{i=1}^{n}(Y_i-\overline{Y})(\hat{Y}_i-\overline{Y})}{\sqrt{\sum_{i=1}^{n}(Y_i-\overline{Y})^2\sum_{i=1}^{n}(\hat{Y}_i-\overline{Y})^2}}$$
After some algebraic manipulation, we have:

$$r_{Y,\hat{Y}}=\sqrt{R^2}$$

# Units of measurement and functional form

<div style="margin-left: auto;
            margin-right: auto;
            width: 80%">

Model      | Dependent Variable|Independent Variable|Interpretation of $\beta_1$
-----------|:-----------------:|:------------------:|:-------------------------:
Level-level|$y$                |$x$                 |${\Delta}y={\beta_1\Delta}x$
Level-log  |$y$                |$log(x)$            |${\Delta}y=({\beta_1/100)\%\Delta}x$
Log-level  |$log(y)$           |$x$                 |$\%{\Delta}y=(100{\beta_1}){\Delta}x$
Log-log    |$log(y)$           |$log(x)$            |$\%{\Delta}y={\beta_1}\%{\Delta}x$

Table: Summary of Functional Forms Involving Logarithms
</div>

# Simulation

Simulation of $\beta_1$:

```{r}
x <- rnorm(250, mean=0, sd=3)        # variable x, with no. of observation = 250
m <- 10000                           # 10000 regressions is simulated

beta1hat <- vector(length = m)       # create an empty vector
for(n in 1:m){
  u <- rnorm(250, mean=0, sd=6)      # generate the error term
  y <- 3+2*x+u                       # define dependent variable y
  OLS <- lm(y~x)
  beta1hat[n] <- OLS$coefficients[2]
}
hist(beta1hat)
```


# References
