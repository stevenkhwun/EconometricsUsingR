---
title: "Simple regression model"
output: bookdown::html_document2
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

**Key terms**

* Dependent variable
* Explanatory variable, independent variable, regressor
* Error term, disturbance
* Residual
* Residual sum of squares, _RSS_
* Ordinary least squares, _OLS_
* Normal equations
* Demeaning
* Goodness of fit
* Total sum of squares, _TSS_
* Explained sum of squares, _ESS_
* Coefficient of determination: $R^2$


# The simple regression model

## The model

Equation \@ref(eq:slm), which is assumed to hold in the population of interest, defines the **simple linear regression model**.

\begin{equation}
y=\beta_{0}+\beta_{1}x+u
(\#eq:slm)
\end{equation}


where $y$ is usually described as the **dependent variable**, and $x$ as the **explanatory variable** or **independent variable** or the **regressor**. In statistical analysis, one generally acknowledges the fact that the relationship is not exact by explicitly including in it a random factor known as the **error term** or **disturbance**, $u$.\
\
The simple regression model implies that $x$ has a linear effect on $y$:
$${\Delta}y = \beta_{1}{\Delta}x$$
if ${\Delta}u=0$.\
\
In other words, the marginal effect of $x$ on $y$ is constant and equal to $\beta_1$.


## The fitted model

After the parameters, $\beta_0$ and $\beta_1$, are estimated, the fitted model can be represented as follows:
\begin{equation}
\hat{y_{i}}=\hat{\beta_{0}}+\hat{\beta_{1}}x_{i}
(\#eq:slm-fitted)
\end{equation}

The difference between the actual value of $y_{i}$ and the fitted value $\hat{y_{i}}$, in observation $i$ is known as the **residual** in the observation $i$, $\hat{u}_{i}$. It can be expressed as:

\begin{equation}
\hat{u}_{i}=y_{i}-\hat{y_{i}}
(\#eq:residual)
\end{equation}

## Assumptions of the error term, $u$
There are two assumptions on the *error term* of the model:

1.  As long as the intercept $\beta_{0}$ is included in the equation, we assume
\begin{equation}
E(u)=0
(#eq:u-A1)
\end{equation}
This is an innocuous normalisation and can be imposed without a loss of generality. However, it says nothing about the relationship between $u$ and $x$.

2.  The second key assumption is on the expected vaue of $u$ given $x$ or, in other words, on the mean of the error term for each slice of the population determined by the values of $x$, that is:
\begin{equation}
E(u|x)=E(u) ~~ for ~~ all ~~ values ~~ of ~~ x
(#eq:u-A2)
\end{equation}
This implies that $u$ is **mean independent** of $x$, or $u$ is uncorrelated with $x$.\
\
Combining with the first assumption, we have
\begin{equation}
E(u|x)=E(u)=0 ~~ for ~~ all ~~ values ~~ of ~~ x
(#eq:u-A2-1)
\end{equation}
This is called the **zero conditional mean assumption**. It says that the expected value of the error term, $u$, is zero, regardless of what the value of the explanatory variable, $x$, is.


By the assumptions of the *error terms*, especially the property of the conditional expectation, $E(u|x)=0$, implies:

\begin{eqnarray}
E(y|x) &=& E(\beta_0+{\beta_1}x+u|x) \\
&=& \beta_0+{\beta_1}x+E(u|x) \\
&=& \beta_0+{\beta_1}x \\
(#eq:prf)
\end{eqnarray}

This shows that the **population regression function**, $E(y|x)$, has the following properties:

*  it is a linear function of x
*  an one-unit change in $x$ changes the expected value of $y$ by the amount of $\beta_1$
*  the distribution of $y$ is centered about $E(y|x)$
*  it gives us a relationship between the average level of $y$ at different levels of $x$


# Derivation of the regression coefficients

## Least squares approach

We use the **least squares** criterion to choose $\hat{\beta_{1}}$ amd $\hat{\beta_{2}}$ so as to minimize _**RSS**_, the **residual sum of squares** (sum of the squares of the residuals).

$$RSS=\sum_{i=1}^{n}{\hat{u}_{i}}^2$$
This is usually referred to as **ordinary least squares** and abbreviated _**OLS**_.  
<br />
The square of the residual in observation $i$ in terms of $\hat{\beta_{1}}$ and $\hat{\beta_{2}}$, and the data on $X$ and $Y$:


\begin{eqnarray}
{\hat{u}_{i}}^2 &=&(Y_i-\hat{Y}_{i})^2 =(Y_i-\hat{\beta}_{1}-\hat{\beta}_{2}X_i)^2 \\

&=& Y_i^2+\hat{\beta}_{1}^2+\hat{\beta}_{2}^2X_i^2-2\hat{\beta}_{1}Y_i-2\hat{\beta}_{2}X_iY_i+2\hat{\beta}_{1}\hat{\beta}_{2}X_i \\
\end{eqnarray}


<br />


Summing over all the $n$ observations, we can write _RSS_ as

$$RSS=\sum_{i=1}^{n}Y_i^2+n\hat{\beta}_{1}^2+\hat{\beta}_{2}^2\sum_{i=1}^{n}X_i^2-2\hat{\beta}_{1}\sum_{i=1}^{n}Y_i-2\hat{\beta}_{2}\sum_{i=1}^{n}X_iY_i+2\hat{\beta}_{1}\hat{\beta}_{2}\sum_{i=1}^{n}X_i$$

We find the particular values of $\hat{\beta}_{1}$ and $\hat{\beta}_{2}$ by minimize _RSS_. The partial differentials of _RSS_ with respect to $\hat{\beta}_{1}$ and $\hat{\beta}_{2}$ are:

$$\frac{\partial{RSS}}{\partial{\hat{\beta}_{1}}}=2n\hat{\beta}_{1}-2\sum_{i=1}^{n}Y_i+2\hat{\beta}_{2}\sum_{i=1}^{n}X_i$$

$$\frac{\partial{RSS}}{\partial{\hat{\beta}_{2}}}=2\hat{\beta}_{2}\sum_{i=1}^{n}X_i^2-2\sum_{i=1}^{n}X_iY_i+2\hat{\beta}_{1}\sum_{i=1}^{n}X_i$$

The values of $\hat{\beta}_{1}$ and $\hat{\beta}_{2}$ that minimize _RSS_ must satisfy the first-order conditions

\begin{equation}
\frac{\partial{RSS}}{\partial{\hat{\beta}_{1}}}=0
\quad\mathrm{and}\quad
\frac{\partial{RSS}}{\partial{\hat{\beta}_{2}}}=0
\end{equation}



Hence
$$2n\hat{\beta}_{1}-2\sum_{i=1}^{n}Y_i+2\hat{\beta}_{2}\sum_{i=1}^{n}X_i=0$$
$$2\hat{\beta}_{2}\sum_{i=1}^{n}X_i^2-2\sum_{i=1}^{n}X_iY_i+2\hat{\beta}_{1}\sum_{i=1}^{n}X_i=0$$
These equations are known as the **normal equations** for the regression coefficients.  
<br />
Solving the normal equations and noting that

\begin{equation}
\overline{X}=\frac{1}{n}\sum_{i=1}^{n}X_i
\quad\mathrm{and}\quad
\overline{Y}=\frac{1}{n}\sum_{i=1}^{n}Y_i
\end{equation}

and hence

\begin{equation}
\hat{\beta}_{0}=\overline{Y}-\hat{\beta}_{1}\overline{X}
(\#eq:intercept)
\end{equation}

\begin{equation}
\hat{\beta}_{1}=\frac{\sum_{i=1}^{n}X_iY_i-n\overline{X}\overline{Y}}{\sum_{i=1}^{n}X_i^2-n\overline{X}^2}=\frac{\sum_{i=1}^{n}(X_i-\overline{X})(Y_i-\overline{Y})}{\sum_{i=1}^{n}(X_i-\overline{X})^2}
(\#eq:slope)
\end{equation}

## Method of moments approach





# An example

The following example is adapted from an example in p. 98 of [@Dougherty]. The variable `EARNING`, hourly earnings in 2011 measured in dollars, is regressed on schooling, `S`, measured as highest grade completed, for 500 respondents from the US National Longitudinal Survey of Youth 1997-.
```{r}
# EARNINGS regress on S
eawe21 <- read.csv('Data/EAWE21.csv', header=T)     # Import data
lin.model <- lm(EARNINGS ~ S, data = eawe21)        # Run the model
summary(lin.model)                                  # The result
```

Apart from the regression output above, the values of the estimated coefficients can also be obtained by the `R` code `lin.model$coef[1]` for $\beta_0$, that is `r lin.model$coef[1]`, and `lin.model$coef[2]` for $\beta_1$, that is `r lin.model$coef[2]`.\

As a result, the fitted equation is:

$$\hat{EARNINGS}=0.76+1.27S$$
Interpreting the equation literally, the slope coefficient indicates that, as `S` increases by one unit, `EARNINGS` increases by 1.27 units. This implies that hourly earnings increase by $1.27 for every extra year of schooling.  
<br />
Figure \@ref(fig:simple-wage), which is produced by the following `R` codes, is  a plot of the data together with the regression line. It is a replicate of Figure 1.7 on p. 99 of [@Dougherty].

```{r simple-wage, fig.cap = "A simple wage equation"}
# EARNINGS regress on S
# Plot the data
plot(EARNINGS ~ S, 
     data = eawe21,
     main = "A simple wage equation", 
     xlab = "Years of schooling (highest grade completed)",
     ylab = "Hourly earnings ($)",
     xlim = c(0, 20),
     ylim = c(0, 120))

abline(lin.model)              # Add the regression line

```


## Demeaning
Often the intercept in a regression equation has no sensible interpretation because $X=0$ is distant from the data range. The wage equation above is an example, with the intercept being implausibly low.  
<br />
Sometimes it is useful to deal with the problem by defining $X^*$ as the deviation of $X$ about its sample mean

$$X_{i}^{*}=X_i-\overline{X}$$
and regressing $Y$ on $X^*$. The slope coefficient will not be affected, but the intercept will now be the fitted value of $Y$ at the sample mean of $X$.  
<br />
The following R code gives the descriptive statistics of `S`. This gives the mean years of schooling was 14.87.
```{r}
# Descriptive statistics for S
summary(eawe21$S)
```
We then regress `EARNINGS` on the demeaned variable `SDEV`, where
$$SDEV_i=S_i-\overline{S}$$
```{r, message=FALSE, warning=FALSE}
# Create the demeaned variable SDEV
library(tidyverse)                  # Load the required package

eawe21.dm <- eawe21 %>%
  mutate(SDEV = S - mean(S)) %>%    # Create the demeaned variable
  select(EARNINGS, S, SDEV)         # Select the required columns

# EARNINGS regress on the demeaned variable SDEV
# Using pipe (%>%) operator to simplify the code
lm(EARNINGS ~ SDEV, data = eawe21.dm) %>%        # Run the model
  summary                                        # Print the result      
```
From the regression results, we can see that the coefficient for `SDEV` is 1.27, which is the same as the coefficient of `S` in the original wage equation. However, this regression further predicts that the hourly earnings is $19.58 for the mean years of schooling of 14.87.


# Some important results relating to OLS regressions

These results are valid only if the model includes an intercept. These results hold automatically, irrespective of whether the model is well or poorly specified.

1.  The mean value of the residuals is zero
$$\overline{\hat{u}}=0$$
As a corollary, the mean of the fitted values of $Y$ is equal to the mean of the actual values of $Y$.
$$\overline{\hat{Y}}=\overline{Y}$$
2.  The sample covariance^[By definition, $Cov(X,Y)=E[(X-E[X])(Y-E[Y])]$. Hence, $Cov(X,\hat{u})=E[(X-E[X])(\hat{u}-E[\hat{u}])]=\frac{1}{n}\sum_{i=1}^{n}[(X_i-E[X])(\hat{u}_i-E(\hat{u})]$.] between the observations on $X$ and the residuals is zero. 
Using the normal equation, we can show that $\sum{}{}X_i\hat{u}_i=0$. Together with the fact that $\overline{\hat{u}}=0$, we can show that the sample covariance between $X$ and $\hat{u}$ is $0$, that is:
$$\frac{1}{n}\sum_{i=1}^{n}(X_i-\overline{X})(\hat{u}_i-\overline{\hat{u}})=0$$
Hence the correlation coefficient^[The correlation of two random variables $X$ and $Y$, denoted by $\rho(X,Y)$, is defined as $\rho(X,Y)=\frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}$] is also zero, provided that the sample variances of $X$ and $\hat{u}$ are both nonzero.  
As a corollary, the fitted values of the dependent variable are uncorrelated with the residuals in a simple regression model.

3.  The fitted line must pass through the point {$\overline{X}$,$\overline{Y}$} representing the mean of the observations in the sample.


# Goodness of fit

## Coefficient of determination $R^2$
The variations in $Y$ in any sample can be summarized by
$$TSS=\sum(Y_i-\overline{Y})^2$$
where **_TSS_** is the **total sum of squares**, the sum of the squared deviations about its sample mean.  
<br />
After fitting a regression:
$$Y_i=\hat{Y}_i+\hat{u}_i$$
We can use this to decompose $\sum(Y_i-\overline{Y})^2$:
$$\sum_{i=1}^{n}(Y_i-\overline{Y})^2=\sum_{i=1}^{n}([\hat{Y}_i+\hat{u}_i]-[\overline{\hat{Y}}+\overline{\hat{u}}])^2$$
After some algebraic manipulation, we have:
$$\sum_{i=1}^{n}(Y_i-\overline{Y})^2=\sum_{i=1}^{n}(\hat{Y}_i-\overline{Y})^2+\sum_{i=1}^{n}\hat{u}_i^2$$
Thus we have the decomposition
$$TSS=ESS+RSS$$
where **_ESS_**, the **explained sum of squares**, and **_RSS_**, the **residual sum of squares**, are the two terms on the right side.  
<br />


The proportion of the total sum of squares explained by the regression line is known as the **coefficient of determination** or, more usually, $R^2$:

$$R^2=\frac{ESS}{TSS}=\frac{\sum_{i=1}^{n}(\hat{Y}_i-\overline{Y})^2}{\sum_{i=1}^{n}(Y_i-\overline{Y})^2}$$
The value of _ESS_ and _RSS_ are provided by the command `anova()` in `R`. 
```{r}
# EARNINGS regress on S
# Analysis of variance table
anova(lin.model)
```
_ESS_, which is described as the independent variable `S` sum of squares, is $6,014$. _TSS_ is the sum of _ESS_ and the _RSS_, which is described as `Residual` sum of squares, is 64,315. Hence, $TSS=6,014+64,315=70,329$. 
Dividing _ESS_ by _TSS_, we have $R^2=0.08551$, which is given by the command `summary()` in `R`.
```{r}
# EARNINGS regress on S
# Summary of regression results, including the coefficient of determination
summary(lin.model)
```
Other things being equal, one would like $R^2$ to be as high as possible. In particular, we would like the coefficients $\hat{\beta}_1$ and $\hat{\beta}_2$ should be chosen in such a way as to maximize $R^2$. On the other hand, the coefficients $\hat{\beta}_1$ and $\hat{\beta}_2$ should be chosen to minimize the sum of the squares of the residuals. In fact, the two criteria are equivalent since we can rewrite $R^2$ as:
$$R^2=1-\frac{RSS}{TSS}=1-\frac{\sum_{i=1}^{n}\hat{u}_i^2}{\sum_{i=1}^{n}(Y_i-\overline{Y})^2}$$

Since $\hat{\beta}_1$ and $\hat{\beta}_2$ minimize the residual sum of squares, they automatically maximize $R^2$.  
<br />

## Alternative interpretation of $R^2$
It should be intuitively obvious that the better is the fit achieved by the regression equation, the higher should be the correlation coefficient for the actual and predicted values of $Y$, which the correlation coefficient is denoted as $r_{Y,\hat{Y}}$:

$$r_{Y,\hat{Y}}=\frac{\sum_{i=1}^{n}(Y_i-\overline{Y})(\hat{Y}_i-\overline{Y})}{\sqrt{\sum_{i=1}^{n}(Y_i-\overline{Y})^2\sum_{i=1}^{n}(\hat{Y}_i-\overline{Y})^2}}$$
After some algebraic manipulation, we have:

$$r_{Y,\hat{Y}}=\sqrt{R^2}$$

# Units of measurement and functional form

<div style="margin-left: auto;
            margin-right: auto;
            width: 80%">

Model      | Dependent Variable|Independent Variable|Interpretation of $\beta_1$
-----------|:-----------------:|:------------------:|:-------------------------:
Level-level|$y$                |$x$                 |${\Delta}y={\beta_1\Delta}x$
Level-log  |$y$                |$log(x)$            |${\Delta}y=({\beta_1/100)\%\Delta}x$
Log-level  |$log(y)$           |$x$                 |$\%{\Delta}y=(100{\beta_1}){\Delta}x$
Log-log    |$log(y)$           |$log(x)$            |$\%{\Delta}y={\beta_1}\%{\Delta}x$

Table: Summary of Functional Forms Involving Logarithms
</div>

# Assumptions for regression models with nonstochastic regressors

This list of assumptions, excluding Assumption 6, constitutes the **classical linear regression model**.^[See Robert S. Pindyck and Daniel L. Rubinfeld, _Econometric Models and Economic Forecasts_, McGraw-Hill, Singapore, 1991, pp. 47.]  
<br />


## Assumption 1
The regression model is **linear in the parameters**, though it may or may not be linear in the variables. That is the regression model as shown in the following equation:

$$Y_{i}=\beta_{1}+\beta_{2}X_{i}+u_{i}$$
"Linear in parameters" means that each term on the right side includes a $\beta$ parameter as a simple factor and there is no built-in relationship among the $\beta$ parameters.\

## Assumption 2
Values taken by the regressor $X$ may be considered fixed in repeated samples. This is equivalent to the assumption that the independent variable in question is controlled by the researcher, who can change its value in accordance with experimental objectives. Such an assumption is unrealistic in the study of most business and economic problems; it has been made for expositional purposes.\

## Assumption 3
Given the value of $X_i$, the mean, or expected, value of the random disturbance term $u_i$ is zero. Symbolically, we have

$$E(u_i|X_i,...,X_n)=0 ~~~ for~all ~~ i$$
To simplify the appearance of the algebra, we will take the conditionality for granted and not write it explicitly each time.
$$E(u_i)=0 ~~~ for~all ~~ i$$
Note that the assumption $E(u_i|X_i)=0$ implies that $E(Y_i|X_i)=\beta_1+\beta_2X_i$. (Why?) Note also that if the conditional mean of one variable given another random variable is zero, the covariance between the two variables is zero and hence the two variables are uncorrelated. Assumption 3 therefore implies that $X_i$ and $u_i$ are uncorrelated.^[The converse, however, is not true because correlation is a measure of linear association only. That is, even if $X_i$ and $u_i$ are uncorrelated, the conditional mean of $u_i$ given $X_i$ may not be zero. However, if $X_i$ and $u_i$ are correlated, $E(u_i|X_i)$ must be nonzero, violating Assumption 3. See James H. Stock and Mark W, Watson, _Introduction to Econometrics_, Addison-Wesley, Boston, 2003, pp. 104--105.]

It is important to point out that Assumption 3 implies that there is no **specification bias** or **specification error** in the model used in empirical analysis. In other words, the regression model is correctly specified.

Actually, it is usually reasonable to assume that Assumption 3 is satisfied automatically if an intercept is included in the regression equation. This is because the role of the intercept is to pick up any systematic, but constant, tendency in $Y$ not accounted for by the explanatory variables included in the regression equation.  
<br />

## Assumption 4
The disturbance term is **homoskedastic**. This means that its value in each observation is drawn from a distribution with constant population variance:

$$E\{(u_i-\mu_u)^2\}=\sigma_u^2 ~~~ for~all~~i$$
Since $E(u_i)=\mu_u=0$ by virtue of Assumption 3, the condition may also be written

$$E(u_i^2)=\sigma_u^2 ~~~ for~all~~i$$
Note that Assumption 4 implies that the conditional variances of $Y_i$ are also homoscedastic.^[See Damodar N. Gujarati and Dawn C. Porter, _Basic Econometrics_, McGraw-Hill/Irwin, New York, 2009, pp. 66.] That is,
$$var(Y_i|X_i)=\sigma^2$$
Of course, the _unconditional variance_ of $Y$ is $\sigma_Y^2$.

In contrast, where the conditional variance of the $Y$ population varies with $X$, this situation is known appropriately as **heteroscedasticity**, or unequal __spread__, or __variance__. Symbolically,
$$var(u_i|X_i)=\sigma_i^2$$
If Assumption 4 is not satisfied, the OLS regression coefficient will be inefficient.  
<br />

## Assumption 5
The values of the disturbance term have independent distributions. That is, $u_i$ is distributed independently of $u_j$ for all $j \ne i$. In other words, the disturbance term is not subject to **autocorrelation**, meaning that there should be no systematic association between its values in any two observations. 

The assumption implies that $\sigma_{u_iu_j}$, the population covariance between $u_i$ and $u_j$, is zero, because

\begin{eqnarray}
\sigma_{u_iu_j} &=& E\{(u_i-\mu_u)(u_j-\mu_u)\} =E(u_iu_j) \\
&=& E(u_i)E(u_j)=0 \\
\end{eqnarray}

(Note that $E(u_iu_j)$ can be decomposed as $E(u_i)E(u_j)$ if $u_i$ and $u_j$ are generated independently.)


If this assumption is not satisfied, OLS will again give inefficient estimates.  
<br />

## Assumption 6

The disturbance term has a **normal distribution**. The justification for the assumption depends on the Lindeberg-Feller central limit theorem. In essence, this states that if a ranom variable is the composite result of the effects of a large number of other random variables, it will have an approximately normal distribution even if its components do not, provided that none of them is dominant. The disturbance term $u$ is composed of a number of factors not appearing explicitly in the regression equation and so, even if we know nothing about the distributions of these factors, we are usually entitled to assume that the disturbance term is normally distributed. (The Lindeberg-Levy central limit theorem required all the random components to be drawn from the same distribution. The Lindeberg-Feller theorem is less restrictive.)



# Simulation

Simulation of $\beta_1$:

```{r}
x <- rnorm(250, mean=0, sd=3)        # variable x, with no. of observation = 250
m <- 10000                           # 10000 regressions is simulated

beta1hat <- vector(length = m)       # create an empty vector
for(n in 1:m){
  u <- rnorm(250, mean=0, sd=6)      # generate the error term
  y <- 3+2*x+u                       # define dependent variable y
  OLS <- lm(y~x)
  beta1hat[n] <- OLS$coef[2]
}
hist(beta1hat)
var(beta1hat)
```

# Simulation 2

Adapted from p. 126 of [@Dougherty].
```{r}
x <- c(1:20)                         # variable x, with no. of observation = 250
m <- 10000                           # 10000 regressions is simulated

beta1hat2 <- vector(length = m)      # create an empty vector
for(n in 1:m){
  u <- rnorm(20, mean=0, sd=1)       # generate the error term
  y <- 2+0.5*x+u                     # define dependent variable y
  OLS <- lm(y~x)
  beta1hat2[n] <- OLS$coef[2]
}
hist(beta1hat2)
var(beta1hat2)
```
# References
